{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import spacy\n",
    "import torch\n",
    "import random\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/87/9qwg67f164gck_jk0qq9fkc40000gn/T/ipykernel_96615/1352832593.py:12: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  du = pd.read_csv(\"sep_dec_2022_pol.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12879236\n"
     ]
    }
   ],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "device_id = 0 if torch.backends.mps.is_available() else -1\n",
    "\n",
    "model_name = \"roberta-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name).to(device)\n",
    "fill_mask_pipeline = pipeline(\"fill-mask\", model=model_name, device=device_id)\n",
    "\n",
    "du = pd.read_csv(\"sep_dec_2022_pol.csv\") \n",
    "print (len(du))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12879224\n"
     ]
    }
   ],
   "source": [
    "du = du.dropna(subset = ['comment'])\n",
    "print (len(du))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12732657"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get threads with three unique posters\n",
    "threads_with_3plus_users = du.groupby(\"thread_num\")[\"stuff\"].nunique()\n",
    "du = du[du[\"thread_num\"].isin(threads_with_3plus_users[threads_with_3plus_users >= 3].index)]\n",
    "len(du) #12,732,657"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/87/9qwg67f164gck_jk0qq9fkc40000gn/T/ipykernel_96615/4192613277.py:6: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df_filtered = df[~df[\"comment\"].str.contains(bible_regex, case=True, regex=True, na=False)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12732648"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop too much punctuation and bible spam \n",
    "\n",
    "def drop_bible_verses(df):\n",
    "    # i used chatgpt for this man idk a better way to do this? \n",
    "    bible_regex = r'\\b(?:Genesis|Gen|Exodus|Exod|Leviticus|Lev|Numbers|Num|Deuteronomy|Deut|Joshua|Josh|Judges|Judg|Ruth|1 Samuel|1 Sam|2 Samuel|2 Sam|1 Kings|1 Kgs|2 Kings|2 Kgs|1 Chronicles|1 Chr|2 Chronicles|2 Chr|Ezra|Nehemiah|Neh|Esther|Est|Job|Psalms|Psalm|Ps|Proverbs|Prov|Ecclesiastes|Eccl|Song of Solomon|Song|Isaiah|Isa|Jeremiah|Jer|Lamentations|Lam|Ezekiel|Ezek|Daniel|Dan|Hosea|Hos|Joel|Amos|Obadiah|Obad|Jonah|Micah|Nahum|Habakkuk|Hab|Zephaniah|Zeph|Haggai|Hag|Zechariah|Zech|Malachi|Mal|Matthew|Matt|Mark|Mk|Luke|Lk|John|Jn|Acts|Romans|Rom|1 Corinthians|1 Cor|2 Corinthians|2 Cor|Galatians|Gal|Ephesians|Eph|Philippians|Phil|Colossians|Col|1 Thessalonians|1 Thess|2 Thessalonians|2 Thess|1 Timothy|1 Tim|2 Timothy|2 Tim|Titus|Philemon|Phlm|Hebrews|Heb|James|Jas|1 Peter|1 Pet|2 Peter|2 Pet|1 John|2 John|3 John|Jude|Revelation|Rev) \\d+:\\d+(-\\d+)?\\b'\n",
    "    df_filtered = df[~df[\"comment\"].str.contains(bible_regex, case=True, regex=True, na=False)]\n",
    "    return df_filtered\n",
    "\n",
    "du = drop_bible_verses(du)\n",
    "len(du) #12,732,648"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/87/9qwg67f164gck_jk0qq9fkc40000gn/T/ipykernel_96615/303654199.py:3: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  mask = df[\"comment\"].astype(str).str.contains(repeated_word_regex, regex=True, na=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12705745"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def drop_repeated_word_spam(df):\n",
    "    repeated_word_regex = r'(?i)\\b(\\w+)(?:\\s+\\1){2,}\\b'\n",
    "    mask = df[\"comment\"].astype(str).str.contains(repeated_word_regex, regex=True, na=False)\n",
    "    return df.loc[~mask] \n",
    "\n",
    "du = drop_repeated_word_spam(du)\n",
    "len(du) #12,705,745"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_high_punctuation_comments(df, threshold=0.075):\n",
    "    def punctuation_ratio(text):\n",
    "        if not isinstance(text, str) or len(text) == 0:\n",
    "            return 0  \n",
    "        num_punctuation = sum(1 for char in text if char in string.punctuation)\n",
    "        total_chars = len(text)\n",
    "        return num_punctuation / total_chars if total_chars > 0 else 0  \n",
    "    df[\"punctuation_ratio\"] = df[\"comment\"].astype(str).apply(punctuation_ratio)\n",
    "    df_filtered = df[df[\"punctuation_ratio\"] <= threshold].drop(columns=[\"punctuation_ratio\"])\n",
    "    return df_filtered\n",
    "\n",
    "du = drop_high_punctuation_comments(du)\n",
    "print(len(du)) #12,021,537"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12008204"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threads_with_3plus_users = du.groupby(\"thread_num\")[\"stuff\"].nunique()\n",
    "du = du[du[\"thread_num\"].isin(threads_with_3plus_users[threads_with_3plus_users >= 3].index)]\n",
    "len(du) #12,008,204"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12008204\n",
      "10993664\n"
     ]
    }
   ],
   "source": [
    "du = du.dropna(subset=[\"comment\"])\n",
    "print (len(du)) #12,008,204\n",
    "du = du.drop_duplicates(subset = ['comment'])\n",
    "print (len(du)) #10,993,664"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10971948"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threads_with_3plus_users = du.groupby(\"thread_num\")[\"stuff\"].nunique()\n",
    "du = du[du[\"thread_num\"].isin(threads_with_3plus_users[threads_with_3plus_users >= 3].index)]\n",
    "len(du) #10,971,948"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "du.to_csv(\"filter_sep_dec_2022_pol.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_thread_dict(df):\n",
    "    thread_dict = {\n",
    "        thread: thread_df.sort_values(by=\"timestamp\")[[\"num\", \"comment\"]].to_dict(orient='records')\n",
    "        for thread, thread_df in df.groupby(\"thread_num\")\n",
    "    }\n",
    "    return thread_dict\n",
    "\n",
    "def precompute_thread_dict(df):\n",
    "    thread_dict = {}\n",
    "    for thread, thread_df in df.groupby(\"thread_num\"):\n",
    "        thread_dict[thread] = thread_df.sort_values(by=\"timestamp\")[[\"num\", \"comment\"]].to_dict(orient='records')\n",
    "    return thread_dict\n",
    "\n",
    "def pretokenize_comments(thread_dict):\n",
    "    for thread, posts in thread_dict.items():\n",
    "        for post in posts:\n",
    "            post['tokenized_comment'] = set(post['comment'].lower().split())  # Store pre-tokenized words\n",
    "    return thread_dict\n",
    "\n",
    "karina = pretokenize_comments(precompute_thread_dict(du))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tokenized_sets_to_lists(thread_dict):\n",
    "    for posts in thread_dict.values():\n",
    "        for post in posts:\n",
    "            if isinstance(post.get('tokenized_comment'), set):\n",
    "                post['tokenized_comment'] = list(post['tokenized_comment'])\n",
    "    return thread_dict\n",
    "\n",
    "# Save to JSON file\n",
    "with open('thread_dict_2022_sep_dec_pol.json', 'w') as f:\n",
    "    json.dump(convert_tokenized_sets_to_lists(karina), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCTUATION_TABLE = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "def fast_clean_word(word):\n",
    "    return word.translate(PUNCTUATION_TABLE).lower().replace(\"â€™\", \"\").replace(\"'\", \"\")\n",
    "\n",
    "def is_recognized_spacy(word):\n",
    "    token = nlp(word)[0]  \n",
    "    return token.has_vector and not token.is_oov  \n",
    "\n",
    "def has_repeating_letters(word):\n",
    "    return bool(re.search(r\"(.)\\1\\1\", word))  \n",
    "\n",
    "def is_pure_english(word):\n",
    "    return bool(re.match(r\"^[a-zA-Z]+$\", word))  \n",
    "\n",
    "def collect_filtered_words(thread_dict, min_threads=180):\n",
    "    word_threads = defaultdict(set)\n",
    "\n",
    "    for thread, posts in thread_dict.items():\n",
    "        for post in posts:\n",
    "            for word in map(fast_clean_word, post['tokenized_comment']):\n",
    "                if word:\n",
    "                    word_threads[word].add(thread)\n",
    "\n",
    "    word_thread_counts = {word: len(threads) for word, threads in word_threads.items()}\n",
    "\n",
    "    frequent_words = {word: count for word, count in word_thread_counts.items() if count >= min_threads}\n",
    "    \n",
    "    filtered_frequent_words = {word: count for word, count in frequent_words.items() if len(word) > 3 and word.isalpha()}\n",
    "    \n",
    "    recognized_words = {word: count for word, count in filtered_frequent_words.items() if is_recognized_spacy(word)}\n",
    "    unrecognized_words = {word: count for word, count in filtered_frequent_words.items() if word not in recognized_words}\n",
    "\n",
    "    filtered_recognized_words = {word: count for word, count in recognized_words.items() if not has_repeating_letters(word) and is_pure_english(word)}\n",
    "    filtered_unrecognized_words = {word: count for word, count in unrecognized_words.items() if not has_repeating_letters(word) and is_pure_english(word)}\n",
    "\n",
    "    return filtered_recognized_words, filtered_unrecognized_words, word_thread_counts\n",
    "\n",
    "filtered_recognized_words, filtered_unrecognized_words, word_thread_counts = collect_filtered_words(karina, min_threads=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21472"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_recognized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_words_to_file(word_set, filename):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        for word in sorted(word_set):  # Sort for consistency\n",
    "            f.write(word + \"\\n\")\n",
    "\n",
    "save_words_to_file(filtered_recognized_words, \"2022_sep_dec_pol_recognized_words.txt\")\n",
    "save_words_to_file(filtered_unrecognized_words, \"2022_sep_dec_pol_unrecognized_words.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
