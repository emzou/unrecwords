{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import time \n",
    "from functools import lru_cache\n",
    "import random\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import entropy, spearmanr\n",
    "from scipy.stats import wasserstein_distance\n",
    "from scipy.stats import entropy as scipy_entropy\n",
    "from scipy.stats import entropy, kurtosis, skew\n",
    "from scipy.spatial.distance import jensenshannon, cosine\n",
    "import regex as re\n",
    "import spacy\n",
    "import string\n",
    "import os\n",
    "import cProfile\n",
    "import pstats\n",
    "import io\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you end up doing this, let me know about your machine specs and i can help you figure it out. im actually not sure \n",
    "# how much slower this would be on a CPU, i can also ask jen to maybe give you gcp access? (i haven't gotten around to figuring it myself)\n",
    "pd.options.mode.chained_assignment = None\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "device_id = 0 if torch.backends.mps.is_available() else -1\n",
    "\n",
    "model_name = \"roberta-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name).to(device)\n",
    "fill_mask_pipeline = pipeline(\"fill-mask\", model=model_name, device=device_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "du = pd.read_csv(\"pol_jan_mar_2023.csv\")  #input the correct dataframe here \n",
    "du['timestamp'] = du['converted_timestamp'].apply(pd.to_datetime) # i forgot why i needed to do this might not need \n",
    "print (len(du))\n",
    "\n",
    "# also load in the correct files\n",
    "with open(\"cut_recognized_words.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    recognized_words_list = [line.strip() for line in file]\n",
    "\n",
    "print (len(recognized_words_list))\n",
    "\n",
    "with open(\"cut_unrecognized_words.txt\", \"r\", encoding = \"utf-8\") as file: \n",
    "    unrecognized_words_list = [line.strip() for line in file]\n",
    "\n",
    "print (len(unrecognized_words_list))\n",
    "\n",
    "rec_sublist = [recognized_words_list[i:i + 500] for i in range(0, len(recognized_words_list), 500)] #just so we dont have a million outputs\n",
    "unrec_sublist = [unrecognized_words_list[i:i + 50] for i in range(0, len(unrecognized_words_list), 50)]\n",
    "\n",
    "function_words_spacy = sorted(nlp.Defaults.stop_words)\n",
    "function_words_spacy = set()\n",
    "for word in nlp.Defaults.stop_words:  # Use spaCy's stop words\n",
    "    token = nlp(word)[0]  # Process the word\n",
    "    if token.pos_ in {\"PRON\", \"DET\", \"ADP\", \"CCONJ\", \"SCONJ\", \"AUX\", \"PART\", \"ADV\", \"INTJ\"}:\n",
    "        function_words_spacy.add(word)\n",
    "\n",
    "function_words_spacy = sorted(function_words_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a thread dictionary\n",
    "def df_to_thread_dict(df):\n",
    "    thread_dict = {\n",
    "        thread: thread_df.sort_values(by=\"timestamp\")[[\"num\", \"comment\"]].to_dict(orient='records')\n",
    "        for thread, thread_df in df.groupby(\"thread_num\")\n",
    "    }\n",
    "    return thread_dict\n",
    "\n",
    "def precompute_thread_dict(df):\n",
    "    thread_dict = {}\n",
    "    for thread, thread_df in df.groupby(\"thread_num\"):\n",
    "        thread_dict[thread] = thread_df.sort_values(by=\"timestamp\")[[\"num\", \"comment\"]].to_dict(orient='records')\n",
    "    return thread_dict\n",
    "\n",
    "def pretokenize_comments(thread_dict):\n",
    "    for thread, posts in thread_dict.items():\n",
    "        for post in posts:\n",
    "            post['tokenized_comment'] = set(post['comment'].lower().split())  # Store pre-tokenized words\n",
    "    return thread_dict\n",
    "\n",
    "karina = pretokenize_comments(precompute_thread_dict(du))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# could also look into getting past first appearances (i just chose it for uniformity, but we might expect variation so could be cool)\n",
    "def filter_and_extract_word_stats(thread_dict, target_word):\n",
    "    if not isinstance(target_word, str):\n",
    "        raise ValueError(f\"Invalid target_word input: expected string, got {type(target_word)}\")\n",
    "    word_pattern = re.compile(rf'\\b{re.escape(target_word)}\\b', re.IGNORECASE)\n",
    "    data = []\n",
    "    for thread_id, comments in thread_dict.items():\n",
    "        appearances = (c for c in comments if word_pattern.search(c['comment']))\n",
    "        appearances_list = list(appearances)\n",
    "        if appearances_list:\n",
    "            first_comment = appearances_list[0]\n",
    "            data.append({\n",
    "                'thread_id': thread_id,\n",
    "                'first_appearance': first_comment['comment'],\n",
    "                'first_id': first_comment['num'],\n",
    "                'length': len(comments)})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def mask_df(a_df, a_word, tokenizer = tokenizer):\n",
    "    if not isinstance(a_word, str):\n",
    "        raise ValueError(f\"Invalid a_word input: expected string, got {type(a_word)}\")\n",
    "    # Ensure 'first_appearance' exists before processing\n",
    "    if \"first_appearance\" not in a_df.columns:\n",
    "        print(f\"Skipping word '{a_word}': 'first_appearance' column not found.\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame to indicate skipping\n",
    "    mask_token = tokenizer.mask_token\n",
    "    word_pattern = re.compile(rf'\\b{re.escape(a_word)}\\b', re.IGNORECASE)\n",
    "\n",
    "    def masker(text):\n",
    "        return word_pattern.sub(mask_token, text) if isinstance(text, str) else text\n",
    "\n",
    "    def trim_long_text(text, limit=450):  \n",
    "        if not isinstance(text, str):\n",
    "            return text\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        if len(tokens) <= limit:\n",
    "            return text\n",
    "        words = text.split()\n",
    "        word_indices = [i for i, w in enumerate(words) if word_pattern.search(w)]\n",
    "        if not word_indices:\n",
    "            return \" \".join(words[:limit])\n",
    "        target_index = word_indices[0]\n",
    "        left, right = max(0, target_index - limit // 2), min(len(words), target_index + limit // 2)\n",
    "        return \" \".join(words[left:right])\n",
    "\n",
    "    # Convert column to string before applying transformations\n",
    "    a_df = a_df.copy()\n",
    "    a_df[\"first_appearance\"] = a_df[\"first_appearance\"].astype(str)\n",
    "\n",
    "    # Apply transformations\n",
    "    a_df[\"first_appearance\"] = a_df[\"first_appearance\"].apply(masker)\n",
    "    a_df = a_df[a_df[\"first_appearance\"].str.contains(mask_token, na=False)]\n",
    "    if a_df.empty:\n",
    "        print(f\"Skipping word '{a_word}': No instances found after masking.\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame if nothing remains\n",
    "    a_df[\"first_appearance\"] = a_df[\"first_appearance\"].apply(trim_long_text)\n",
    "    return a_df\n",
    "\n",
    "def get_random_samples(df, trial_num, sample_size=12):\n",
    "    df_list = []\n",
    "    for i in range(1, trial_num + 1):\n",
    "        sampled_df = df.sample(n=sample_size, random_state=i).copy()\n",
    "        sampled_df[\"Trial\"] = i  # Mark each row with its trial number\n",
    "        df_list.append(sampled_df)\n",
    "    return pd.concat(df_list, ignore_index=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_meaning_unrecognized(\n",
    "    df, mask_column, target_word, fill_mask_pipeline=fill_mask_pipeline, \n",
    "    tokenizer=tokenizer, top_k=5, prob_threshold=0.8\n",
    "):\n",
    "    predictions_list = []\n",
    "    mask_token = tokenizer.mask_token\n",
    "    word_pattern = re.compile(rf'\\b{re.escape(target_word)}\\b', re.IGNORECASE)\n",
    "    punctuation_set = set(string.punctuation)\n",
    "    df[mask_column] = df[mask_column].astype(str)\n",
    "    sentences = df[mask_column].str.strip().tolist()\n",
    "    trial_ids = df[\"Trial\"].tolist()\n",
    "    missing_penalty = np.log(1e-8)\n",
    "    for trial_id in set(trial_ids):\n",
    "        all_seen_words = set()\n",
    "        trial_sentences = [word_pattern.sub(mask_token, sentence.lower()) for sentence, t_id in zip(sentences, trial_ids) if t_id == trial_id]\n",
    "        trial_predictions = {}\n",
    "        for sentence in trial_sentences:\n",
    "            if mask_token not in sentence:\n",
    "                print(f\"Skipping sentence due to missing mask token: {sentence[:50]}...\")\n",
    "                continue\n",
    "            try:\n",
    "                tokenized_sentence = tokenizer(sentence, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "                with torch.no_grad():\n",
    "                    predictions = fill_mask_pipeline(sentence)\n",
    "            except RuntimeError as e:\n",
    "                if \"expanded size of the tensor\" in str(e):\n",
    "                    print(f\"Skipping sentence due to tensor size mismatch: {sentence[:50]}...\")\n",
    "                    continue\n",
    "                else:\n",
    "                    raise e\n",
    "            if not isinstance(predictions, list) or 'token_str' not in predictions[0]:\n",
    "                continue\n",
    "            # !!!!!!! Extract log probabilities\n",
    "            filtered_predictions = {\n",
    "                p['token_str'].strip(): np.log(p['score'])\n",
    "                for p in predictions\n",
    "                if 'token_str' in p and not all(char in punctuation_set for char in p['token_str'])\n",
    "            }\n",
    "            if not filtered_predictions:\n",
    "                continue\n",
    "            # Sort candidates by probability\n",
    "            sorted_candidates = sorted(filtered_predictions.items(), key=lambda x: x[1], reverse=True)\n",
    "            max_log_prob = sorted_candidates[0][1]\n",
    "            # Keep at least top-k, and include words within the threshold\n",
    "            top_candidates = {k: v for k, v in sorted_candidates[:top_k]}\n",
    "            for word, log_prob in sorted_candidates[top_k:]:\n",
    "                if np.exp(log_prob - max_log_prob) >= prob_threshold:  \n",
    "                    top_candidates[word] = log_prob\n",
    "            # Normalize within chosen candidates\n",
    "            log_probs = np.array(list(top_candidates.values()))\n",
    "            exp_probs = np.exp(log_probs - max_log_prob)\n",
    "            normalized_probs = dict(zip(top_candidates.keys(), exp_probs / exp_probs.sum()))\n",
    "            all_seen_words.update(normalized_probs.keys())\n",
    "            # Accumulate seen words and allow them to gain weight over time\n",
    "            for word in all_seen_words:\n",
    "                if word in normalized_probs:\n",
    "                    trial_predictions[word] = trial_predictions.get(word, 0) + normalized_probs[word]\n",
    "                else:\n",
    "                    trial_predictions[word] = trial_predictions.get(word, 0) + np.exp(missing_penalty)  \n",
    "        total_prob = sum(trial_predictions.values())\n",
    "        if total_prob > 0:\n",
    "            trial_predictions = {k: v / total_prob for k, v in trial_predictions.items()}\n",
    "        predictions_list.append(trial_predictions)\n",
    "    return predictions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics === you can also look into additional metrics we might add \n",
    "def get_pos(word):\n",
    "    doc = nlp(word)\n",
    "    return doc[0].pos_ if doc and doc[0].pos_ else \"UNKNOWN\"\n",
    "\n",
    "def mean_reciprocal_rank(dict_list):\n",
    "    reciprocal_ranks = []\n",
    "    for d in dict_list:\n",
    "        if d:\n",
    "            sorted_items = sorted(d.items(), key=lambda x: -x[1])\n",
    "            top_word = sorted_items[0][0]\n",
    "            rank = next((i + 1 for i, (word, _) in enumerate(sorted_items) if word == top_word), len(sorted_items))\n",
    "            reciprocal_ranks.append(1 / rank)\n",
    "    return np.mean(reciprocal_ranks) if reciprocal_ranks else 0.0\n",
    "\n",
    "def prediction_diversity(dict_list, k=5):\n",
    "    unique_predictions = set()\n",
    "    for d in dict_list:\n",
    "        top_k_words = sorted(d, key=d.get, reverse=True)[:k]\n",
    "        unique_predictions.update(top_k_words)\n",
    "    return len(unique_predictions) / (k * len(dict_list))\n",
    "\n",
    "def word_rank_volatility(dict_list):\n",
    "    word_positions = {}\n",
    "    for trial_idx, d in enumerate(dict_list):\n",
    "        sorted_words = sorted(d, key=d.get, reverse=True)\n",
    "        for rank, word in enumerate(sorted_words):\n",
    "            if word not in word_positions:\n",
    "                word_positions[word] = []\n",
    "            word_positions[word].append(rank)\n",
    "    rank_diffs = [np.mean(np.abs(np.diff(positions))) for positions in word_positions.values() if len(positions) > 1]\n",
    "    return np.mean(rank_diffs) if rank_diffs else 0.0\n",
    "\n",
    "def rank_stability(dict_list):\n",
    "    ranks = [{k: rank for rank, (k, _) in enumerate(sorted(d.items(), key=lambda x: -x[1]))} for d in dict_list]\n",
    "    diffs = [sum(abs(ranks[i][k] - ranks[i-1].get(k, len(ranks[i]))) for k in ranks[i]) for i in range(1, len(ranks))]\n",
    "    return np.mean(diffs)\n",
    "\n",
    "def temporal_kl_divergence(dict_list):\n",
    "    kl_divs = [kl_divergence(dict_list[i], dict_list[i-1]) for i in range(1, len(dict_list))]\n",
    "    return np.mean(kl_divs) if kl_divs else 0.0\n",
    "\n",
    "def first_guess_consistency(dict_list):\n",
    "    first_guesses = [get_first_guess([d]) for d in dict_list]\n",
    "    most_common_guess, count = Counter(first_guesses).most_common(1)[0]\n",
    "    return count / len(first_guesses)\n",
    "\n",
    "def distribution_compression(dict_list):\n",
    "    entropies = [entropy(d) for d in dict_list]\n",
    "    return np.mean(entropies) if entropies else 0.0\n",
    "\n",
    "def get_first_guess(dict_list):\n",
    "    guess_counts = Counter()\n",
    "    for d in dict_list:\n",
    "        if d:\n",
    "            best_guess = max(d, key=d.get)  # Get the word with the highest probability\n",
    "            guess_counts[best_guess] += 1\n",
    "    return guess_counts.most_common(1)[0][0] if guess_counts else None\n",
    "\n",
    "def cosine_similarity(p, q):\n",
    "    all_keys = set(p.keys()).union(set(q.keys()))\n",
    "    p_vec = np.array([p.get(k, 0) for k in all_keys])\n",
    "    q_vec = np.array([q.get(k, 0) for k in all_keys])\n",
    "    \n",
    "    if np.all(p_vec == 0) or np.all(q_vec == 0):  # Avoid division by zero\n",
    "        return 0.0\n",
    "    return 1 - cosine(p_vec, q_vec)\n",
    "\n",
    "def wasserstein_similarity(p, q):\n",
    "    all_keys = list(set(p.keys()).union(set(q.keys())))\n",
    "    p_vec = np.array([p.get(k, 0) for k in all_keys])\n",
    "    q_vec = np.array([q.get(k, 0) for k in all_keys])\n",
    "\n",
    "    return wasserstein_distance(p_vec, q_vec)\n",
    "\n",
    "def kl_divergence(p, q):\n",
    "    all_keys = set(p.keys()).union(set(q.keys()))  # Get all unique words across both distributions\n",
    "    p_vec = np.array([p.get(k, 1e-10) for k in all_keys])  # Fill missing words with small value\n",
    "    q_vec = np.array([q.get(k, 1e-10) for k in all_keys])  # Same for q\n",
    "    \n",
    "    return stats.entropy(p_vec, q_vec)  # Now both arrays have same shape\n",
    "\n",
    "def js_divergence(p, q):\n",
    "    p = np.array(list(p.values())) + 1e-10\n",
    "    q = np.array(list(q.values())) + 1e-10\n",
    "    m = 0.5 * (p + q)\n",
    "    return 0.5 * (stats.entropy(p, m) + stats.entropy(q, m))\n",
    "\n",
    "def entropy(p):\n",
    "    p = np.array(list(p.values())) + 1e-10\n",
    "    return stats.entropy(p)\n",
    "\n",
    "def jaccard_similarity(p, q, k=5):\n",
    "    top_p = set(sorted(p, key=p.get, reverse=True)[:k])\n",
    "    top_q = set(sorted(q, key=q.get, reverse=True)[:k])\n",
    "    return len(top_p & top_q) / len(top_p | top_q)\n",
    "\n",
    "def most_common_pos(dict_list):\n",
    "    pos_counts = Counter()\n",
    "    for d in dict_list:\n",
    "        for word in d.keys():\n",
    "            pos_counts[get_pos(word)] += 1\n",
    "    return pos_counts.most_common(1)[0][0]\n",
    "\n",
    "def content_score(word_distribution, function_words_set = function_words_spacy):\n",
    "    function_prob = sum(prob for word, prob in word_distribution.items() if word in function_words_set)\n",
    "    return 1 - function_prob  # Higher means more content words\n",
    "\n",
    "\n",
    "def compute_metrics(dict_list, function_words_set = function_words_spacy):\n",
    "    if not dict_list or len(dict_list) < 2:\n",
    "        return {} \n",
    "\n",
    "    metrics = {\n",
    "        'Cosine_Similarity': np.mean([cosine_similarity(dict_list[i], dict_list[i-1]) for i in range(1, len(dict_list))]),\n",
    "        'Wasserstein_Distance': np.mean([wasserstein_similarity(dict_list[i], dict_list[i-1]) for i in range(1, len(dict_list))]),\n",
    "        'Rank_Stability': rank_stability(dict_list),\n",
    "        'Temporal_Jaccard': np.mean([jaccard_similarity(dict_list[i], dict_list[i-1]) for i in range(1, len(dict_list))]),\n",
    "        'Most_Common_POS': most_common_pos(dict_list),\n",
    "        'First_Guess': get_first_guess(dict_list),\n",
    "        #'Mean_Reciprocal_Rank': mean_reciprocal_rank(dict_list),\n",
    "        'Prediction_Diversity': prediction_diversity(dict_list),\n",
    "        'Temporal_KL_Divergence': temporal_kl_divergence(dict_list),\n",
    "        'First_Guess_Consistency': first_guess_consistency(dict_list),\n",
    "        'Distribution_Compression': distribution_compression(dict_list),\n",
    "        'Content_Score': np.mean([content_score(d, function_words_spacy) for d in dict_list]),  # NEW METRIC\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_word_trials(words, dataframe, n):\n",
    "    results = []\n",
    "    total_words = len(words)\n",
    "    start_time = time.time()\n",
    "    for idx, word in enumerate(words, 1):\n",
    "        word_start_time = time.time()\n",
    "        print(f\"Processing word {idx}/{total_words}: {word}\")\n",
    "        covid = filter_and_extract_word_stats(dataframe, word)\n",
    "        maskcovid = mask_df(covid, word)\n",
    "        trial_df = get_random_samples(maskcovid, n)\n",
    "        first_appearance_dicts = accumulate_meaning_unrecognized(trial_df, 'first_appearance', word)\n",
    "        if not first_appearance_dicts or all(len(trial) < 2 for trial in first_appearance_dicts):\n",
    "            print(f\"Skipping word '{word}' due to insufficient data.\")\n",
    "            continue  \n",
    "        all_words = set().union(*first_appearance_dicts)\n",
    "        aligned_dicts = [{word: trial.get(word, 1e-10) for word in all_words} for trial in first_appearance_dicts]\n",
    "        row = {'Word': word}\n",
    "        metrics = compute_metrics(aligned_dicts)\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            row[f'First_{metric_name}'] = metric_value  \n",
    "        results.append(row)\n",
    "        elapsed_time = time.time() - word_start_time\n",
    "        remaining_time = (time.time() - start_time) / idx * (total_words - idx)\n",
    "        print(f\"Completed {word} in {elapsed_time:.2f} seconds. Estimated time remaining: {remaining_time:.2f} seconds.\")\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_sublists(the_sublists, dataframe, n_trials, folder_name=\"known_round6\", start_idx=1):\n",
    "    if not os.path.exists(folder_name):  \n",
    "        os.makedirs(folder_name)  # Create folder once\n",
    "    for idx, sublist in enumerate(the_sublists, 1):  \n",
    "        if idx < start_idx:\n",
    "            continue  # Skip processing until reaching the desired starting index (sometimes I just paused it to do other stuff)\n",
    "        print(f\"Processing sublist {idx}\")\n",
    "        processed_words = []  # Store processed words to rebuild the dataframe\n",
    "        for word in sublist:\n",
    "            try:\n",
    "                word_df = process_word_trials([word], dataframe, n_trials)  # Process one word at a time\n",
    "                if isinstance(word_df, pd.DataFrame) and not word_df.empty:\n",
    "                    processed_words.append(word_df)  # Append successful results\n",
    "                else:\n",
    "                    print(f\"Skipping word '{word}' due to empty dataframe.\")\n",
    "            except ValueError as e:\n",
    "                print(f\"Skipping word '{word}' due to error: {e}\")  # Skip only the word, not the sublist\n",
    "        if not processed_words:\n",
    "            print(f\"Skipping sublist {idx} as all words failed.\")\n",
    "            continue  # Skip saving if no words were successfully processed\n",
    "        df = pd.concat(processed_words, ignore_index=True)  # Merge valid results\n",
    "        file_path = os.path.join(folder_name, f\"6pmknown_round6_sublist_{idx}.csv\")\n",
    "        df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_and_save_sublists(rec_sublist, karina, 10, start_idx=1)\n",
    "# actually running everything :^D "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
